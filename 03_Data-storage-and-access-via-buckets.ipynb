{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb3ff82-e229-4a95-8d54-b9cd914fbc8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61165195-70f5-45c0-aaad-ffb975ebc6ef",
   "metadata": {},
   "source": [
    "## Pushing new files to bucket from notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974d63f-cf39-4ff6-8544-ac8aa6be5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Define the S3 bucket name and the file paths\n",
    "bucket_name = \"titanic-dataset-test\"\n",
    "train_file_path = \"titanic_train.csv\"\n",
    "test_file_path = \"titanic_test.csv\"\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the training file\n",
    "s3.upload_file(train_file_path, bucket_name, \"data/titanic_train.csv\")\n",
    "\n",
    "# Upload the test file\n",
    "s3.upload_file(test_file_path, bucket_name, \"data/titanic_test.csv\")\n",
    "\n",
    "print(\"Files uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6e7f1-19f9-458c-b7e1-f2702d491cd1",
   "metadata": {},
   "source": [
    "## Check current size and storage costs of bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a71377-900f-4314-9017-d36cb83b2d58",
   "metadata": {},
   "source": [
    "Yu can check the size of an S3 bucket directly from a Jupyter notebook in SageMaker by using the **Boto3** library, which is the AWS SDK for Python. This will allow you to calculate the total size of objects within a specified bucket.\n",
    "\n",
    "Hereâ€™s how you can do it:\n",
    "\n",
    "### Step 1: Set Up the S3 Client and Calculate Bucket Size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c2044c-acb4-48f5-b755-570d1cc4ecbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of bucket 'titanic-dataset-test': 27.08 MB\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your bucket name\n",
    "bucket_name = 'titanic-dataset-test'\n",
    "\n",
    "# Initialize the total size counter\n",
    "total_size_bytes = 0\n",
    "\n",
    "# List and sum the size of all objects in the bucket\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket=bucket_name):\n",
    "    for obj in page.get('Contents', []):\n",
    "        total_size_bytes += obj['Size']\n",
    "\n",
    "# Convert the total size to gigabytes for cost estimation\n",
    "total_size_gb = total_size_bytes / (1024 ** 3)\n",
    "# print(f\"Total size of bucket '{bucket_name}': {total_size_gb:.2f} GB\")\n",
    "\n",
    "# Convert the total size to megabytes for readability\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "print(f\"Total size of bucket '{bucket_name}': {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe444fcd-a4aa-4536-aac0-df1c56a79594",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. **Paginator**: Since S3 buckets can contain many objects, we use a paginator to handle large listings.\n",
    "2. **Size Calculation**: We sum the `Size` attribute of each object in the bucket.\n",
    "3. **Unit Conversion**: The size is given in bytes, so dividing by `1024 ** 2` converts it to megabytes (MB).\n",
    "\n",
    "> **Note**: If your bucket has very large objects or you want to check specific folders within a bucket, you may want to refine this code to only fetch certain objects or folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf5a8c-8f28-42a8-be40-a6f155db96e6",
   "metadata": {},
   "source": [
    "### Step 2: Check storage costs of bucket\n",
    "To estimate the storage cost of your Amazon S3 bucket directly from a Jupyter notebook in SageMaker, you can use the following approach. This method calculates the total size of the bucket and estimates the monthly storage cost based on AWS S3 pricing.\n",
    "\n",
    "**Note**: AWS S3 pricing varies by region and storage class. The example below uses the S3 Standard storage class pricing for the US East (N. Virginia) region as of November 1, 2024. Please verify the current pricing for your specific region and storage class on the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999a7e66-04e3-4577-b7ab-4ca9472e3ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated monthly storage cost: $0.0006\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 Standard Storage pricing for US East (N. Virginia) region\n",
    "# Pricing tiers as of November 1, 2024\n",
    "first_50_tb_price_per_gb = 0.023  # per GB for the first 50 TB\n",
    "next_450_tb_price_per_gb = 0.022  # per GB for the next 450 TB\n",
    "over_500_tb_price_per_gb = 0.021  # per GB for storage over 500 TB\n",
    "\n",
    "# Calculate the cost based on the size\n",
    "if total_size_gb <= 50 * 1024:\n",
    "    # Total size is within the first 50 TB\n",
    "    cost = total_size_gb * first_50_tb_price_per_gb\n",
    "elif total_size_gb <= 500 * 1024:\n",
    "    # Total size is within the next 450 TB\n",
    "    cost = (50 * 1024 * first_50_tb_price_per_gb) + \\\n",
    "           ((total_size_gb - 50 * 1024) * next_450_tb_price_per_gb)\n",
    "else:\n",
    "    # Total size is over 500 TB\n",
    "    cost = (50 * 1024 * first_50_tb_price_per_gb) + \\\n",
    "           (450 * 1024 * next_450_tb_price_per_gb) + \\\n",
    "           ((total_size_gb - 500 * 1024) * over_500_tb_price_per_gb)\n",
    "\n",
    "print(f\"Estimated monthly storage cost: ${cost:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0198b8-3e30-4432-840d-093953a11dcd",
   "metadata": {},
   "source": [
    "**Important Considerations**:\n",
    "\n",
    "- **Pricing Tiers**: AWS S3 pricing is tiered. The first 50 TB per month is priced at `$0.023 per GB`, the next 450 TB at `$0.022 per GB`, and storage over 500 TB at `$0.021 per GB`. Ensure you apply the correct pricing tier based on your total storage size.\n",
    "- **Region and Storage Class**: Pricing varies by AWS region and storage class. The example above uses the S3 Standard storage class pricing for the US East (N. Virginia) region. Adjust the pricing variables if your bucket is in a different region or uses a different storage class.\n",
    "- **Additional Costs**: This estimation covers storage costs only. AWS S3 may have additional charges for requests, data retrievals, and data transfers. For a comprehensive cost analysis, consider these factors as well.\n",
    "\n",
    "For detailed and up-to-date information on AWS S3 pricing, please refer to the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
